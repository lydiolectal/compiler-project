#!/usr/bin/env python3

# TODO:
# - add in indexing info in error message. Ex: tokenization error at line 1, char 12.

import sys, re
from value import Value

class Tokenizer:

    def __init__(self, code):
        self.code = code

    def tokenize(self):

        oparen = re.compile("\A\(")
        cparen = re.compile("\A\)")
        bool = re.compile("\A#(t|f)") #t #f
        integer = re.compile("\A(\+|-)?\d+\.?")
        double = re.compile("\A(\+|-)?\d*\.\d+")
        # quote = re.compile("\A(quote|\')")
        # bc syntactic sugar.
        # ' followed by a sequence of non-whitespace, OR ( <stuff> )
        quote_sugar = re.compile("\A\'(\S+|\(.*)")

        # <symbol>	    -->	 <initial> <subsequent>*
        # <initial>	    -->	 <letter> | ! | $ | % | & | * | / | : | < | = | > | ? | ~ | _ | ^
        #                |<Unicode Lu, Ll, Lt, Lm, Lo, Mn, Nl, No, Pd, Pc, Po, Sc, Sm, Sk, So, or Co>
        #                |	\x <hex scalar value> ;
        # <subsequent>	-->	<initial> | <digit 10> | . | + | - | @ | <Unicode Nd, Mc, or Me>
        # <letter>	    -->	a | b | ... | z | A | B | ... | Z
        # use these minus the weird hex stuff and Unicode. !$%&*/:<=>?~_^

        # symbol regex: very fragile; do not touch. :O
        initial = "[A-Za-z!$%&*\/:<=>?~_^]"
        symbol = re.compile("\A[+-.]|" + initial + "(" + initial + "|[0-9.+-@])*")
        string = re.compile("\A\".*\"")
        comment = re.compile("\A;.*(\n|$)")

        index = 0
        tokens = []
        m_string = ""
        while self.code:

            if oparen.match(self.code):
                # matched string
                m_string = oparen.match(self.code).group()
                value = Value("open", m_string)
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif cparen.match(self.code):
                m_string = cparen.match(self.code).group()
                value = Value("close", m_string)
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif bool.match(self.code):
                m_string = bool.match(self.code).group()
                if m_string == "#t":
                    value = Value("boolean", 1)
                else:
                    value = Value("boolean", 0)
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif double.match(self.code):
                # print("Matched double!")
                m_string = double.match(self.code).group()
                value = Value("float", float(m_string))
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif integer.match(self.code):
                m_string = integer.match(self.code).group()
                if m_string[-1] == ".":
                    value = Value("integer", int(m_string[:-1]))
                else:
                    value = Value("integer", int(m_string))
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif quote_sugar.match(self.code):
                m_string = quote_sugar.match(self.code).group()
                value = Value("quote", m_string)
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif symbol.match(self.code):
                m_string = symbol.match(self.code).group()
                value = Value("symbol", m_string)
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif string.match(self.code):
                m_string = string.match(self.code).group()
                value = Value("string", m_string)
                self.code = self.code[len(m_string):]
                tokens.append(value)

            elif comment.match(self.code):
                m_string = comment.match(self.code).group()
                # ignore comments
                # value = Value("comment", m_string)
                self.code = self.code[len(m_string):]
                # tokens.append(value)

            else:
                # errors if it finds an illegal character in first pass.
                if m_string:
                    print("Tokenization error after {}".format(m_string))
                else:
                    print("Tokenization error at beginning of file.")
                break
                # exit and error.
            self.code = self.code.lstrip()

        return tokens

    def display_tokens(self, tokens):
        for token in tokens:
            print(f"{token.data}:{token.type}")

if __name__ == "__main__":
    source_code = sys.stdin.read()
    tokenizer = Tokenizer(source_code)
    tokens = tokenizer.tokenize()
    tokenizer.display_tokens(tokens)
